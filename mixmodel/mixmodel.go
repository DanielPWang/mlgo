package mixmodel

import (
	"rand"
	"math"
	"fmt"
)

type Vector []float64
type Matrix [][]float64

const maxValue = math.MaxFloat64

//TODO debug

type Clusters struct {
	// Matrix of data points [m x n]
	X Matrix
	// number of clusters
	K int
	// Matrix of posterior probabilities [m x k]
	posteriors Matrix
	// Matrix of Gaussians [k x n]
	Means, Variances Matrix
	// Vector of mixing proportions [k]
	Mixings Vector
	// Likelihood to be maximized
	LogLikelihood float64
}

func summary(X Matrix) (means, variances Vector) {
	m := len(X)
	if m < 2 { return }

	n := len(X[0])
	stats := make([]Summary, n)

	means, variances = make(Vector, n), make(Vector, n)

	for i := 0; i < m; i++ {
		// accumulate statistics for each feature
		for j, x := range X[i] {
			stats[j].Add(x)
		}
	}

	for j, s := range stats {
		means[j] = s.Mean
		variances[j] = s.VarP()
	}

	return
}

// Run runs the k-means algorightm for iter random rounds
func Run(X Matrix, K int, iter int) (clusters *Clusters) {
	// run k-means concurrently
	ch := make(chan *Clusters)
	for i := 0; i < iter; i++ {
		c := Clusters{X:X, K:K}
		go c.Solve(ch)
	}

	// determine best clustering
	maxLikelihood := -maxValue
	for i := 0; i < iter; i++ {
		if c := <-ch; c.LogLikelihood > maxLikelihood {
			clusters = c
			maxLikelihood = c.LogLikelihood
		}
	}
	return
}

// Solve runs the k-means algorithm once with random initialization
// Returns the cost
func (c *Clusters) Solve(chClusters chan<- *Clusters) {
	c.initialize()
	i := 0
	for !c.expectation() {
		c.maximization()
		i++
	}
	fmt.Println("Rounds", i)
	fmt.Println("c.Means", c.Means)
	fmt.Println("c.Mixings", c.Mixings)
	fmt.Println("c.LogLikelihood", c.LogLikelihood)
	chClusters <- c
}

// initialize Gaussians randomly
func (c *Clusters) initialize() {
	means, variances := summary(c.X)
	m, n := len(c.X), len(means)

	c.Means, c.Variances, c.Mixings = make(Matrix, c.K), make(Matrix, c.K), make(Vector, c.K)

	c.posteriors = make(Matrix, m)

	c.LogLikelihood = -math.MaxFloat64

	for i := 0; i < m; i++ {
		c.posteriors[i] = make(Vector, c.K)
	}

	for k := 0; k < c.K; k++ {

		c.Means[k], c.Variances[k] = make(Vector, n), make(Vector, n)

		// use large variance: variance of each feature based on entire data set
		copy(c.Variances[k], variances)

		// use mean of each feature plus some noise
		for j := 0; j < len(means); j++ {
			sd := math.Sqrt(variances[j])
			c.Means[k][j] = means[j] + (rand.Float64() * sd - sd/2)
		}

		// uniform mixing proportions
		c.Mixings[k] = 1/float64(c.K)

	}
	fmt.Println("initial: ", c.Means, c.Variances, c.Mixings)
}

type pdf func(float64) float64

func normPdf(mu, sigma2 float64) func(float64) float64 {
	return func(x float64) float64 {
		d := x-mu
		return 1 / math.Sqrt(2*math.Pi*sigma2) * math.Exp(-d*d / (2*sigma2))
	}
}

// expectation step: assign data points to cluster meanss
// Returns whether the algorithm has converged
func (c *Clusters) expectation() (converged bool) {

	// setup Gaussians
	pnorms := make([][]pdf, c.K)
	for k := 0; k < c.K; k++ {
		pnorms[k] = make([]pdf, len(c.Means[k]))
		for d := 0; d < len(c.Means[k]); d++ {
			pnorms[k][d] = normPdf(c.Means[k][d], c.Variances[k][d])
		}
	}

	// Calculate the posterior probability densities of each data point
	//   being generated by each Gaussian using Bayes theorem
	total := 0.0
	for i, _ := range c.X {
		px := 0.0
		for k := 0; k < c.K; k++ {
			likelihood := 1.0
			for d := 0; d < len(c.X[i]); d++ {
				likelihood *= pnorms[k][d](c.X[i][d])
			}
			p := c.Mixings[k] * likelihood
			c.posteriors[i][k] = p
			px += p
		}
		// normalize posterior
		for k := 0; k < c.K; k++ {
			c.posteriors[i][k] /= px
			total -= math.Log(c.posteriors[i][k])
		}
	}

	fmt.Println("likelihood", total, c.LogLikelihood)
	if total - c.LogLikelihood > 0.01 {
		converged = false
	} else {
		converged = true
	}
	c.LogLikelihood = total
	fmt.Println("converged", converged)

	/*
	posterior2 := func(i int, chIndex chan int) {
		index, min :=  0, maxValue
		for ii := 0; ii < len(c.Means); ii++ {
			// calculate distance
			distance := 0.0
			for j := 0; j < len(c.X[i]); j++ {
				diff := c.X[i][j] - c.Means[ii][j]
				distance += diff * diff
			}
			if distance < min {
				index = ii
				min = distance
			}
		}
		chIndex <- index
	}

	// process examples concurrently
	ch := make(chan int)
	for i, _ := range c.X {
		go assign(i, ch)
	}

	// collect results
	converged = true
	for i, _ := range c.X { 
		if index := <- ch; c.Classes[i] != index {
			c.Classes[i] = index
			converged = false
		}
	}
	*/

	return
}

// maximization step: move cluster meanss to centroids of data points
// Returns the cost
func (c *Clusters) maximization() float64 {

	fmt.Println("before: ", c.Means)

	for k := 0; k < c.K; k++ {
		
		// Compute new mixing proportions
		sum := 0.0
		for i, _ := range c.posteriors {
			sum += c.posteriors[i][k]
		}
		c.Mixings[k] = sum / float64( len(c.posteriors) )

		
		// Compute new means
		for d, _ := range c.Means[k] {
			a, b := 0.0, 0.0
			for i, _ := range c.posteriors {
				p := c.posteriors[i][k]
				a += p * c.X[i][d]
				b += p
			}
			c.Means[k][d] = a / b
		}

		// Compuete new variances
		for d, _ := range c.Variances[k] {
			a, b := 0.0, 0.0
			for i, _ := range c.posteriors {
				p := c.posteriors[i][k]
				diff := c.X[i][d] - c.Means[k][d]
				a += p * (diff * diff)
				b += p
			}
			c.Variances[k][d] = a / b
		}

	}

	fmt.Println("after: ", c.Means)

	/*
	// move cluster meanss
	move := func(ii int, chCost chan float64) {
		means := c.Means[ii]
		errors := c.Errors[ii]
		// zero the coordinates
		for j, _ := range means {
			means[j] = 0
			errors[j] = 0
		}
		// compute centroid
		n := 0.0
		for i, class := range c.Classes {
			if class == ii {
				for j, _ := range means {
					x := c.X[i][j]
					means[j] += x
					errors[j] += x * x
				}
				n++
			}
		}
		cost := 0.0
		for j, _ := range means {
			mean := means[j] / n
			means[j] = mean
			// calculate variance * N using sum of squares formula
			errors[j] -= mean*mean * n
			cost = errors[j]
		}
		chCost <- cost;
	}

	// process cluster centers concurrently
	ch := make(chan float64)
	for ii, _ := range c.Means {
		go move(ii, ch)
	}

	// collect results
	J := 0.0
	for ii := 0; ii < len(c.Means); ii++ {
		J += <-ch;
	}
	J /= float64( len(c.X) )
	*/

	return 0
}

